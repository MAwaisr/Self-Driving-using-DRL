{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a736ccfc",
   "metadata": {},
   "source": [
    "Deep reinforcement learning development algorithm attempt 01 using actor-critic strategy with neural networks <br>\n",
    "DRL-AC-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4945186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing important libraries\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341566ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Actor neural network\n",
    "class ActorNetwork(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dense2 = Dense(64, activation='relu')\n",
    "        self.output_layer = Dense(n_actions, activation='softmax')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Define the Critic neural network\n",
    "class CriticNetwork(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dense2 = Dense(64, activation='relu')\n",
    "        self.output_layer = Dense(1)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Define the DRL Actor-Critic agent\n",
    "class DRLActorCritic:\n",
    "    def __init__(self, state_dim, n_actions, lr_actor=0.001, lr_critic=0.001):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions)\n",
    "        self.critic = CriticNetwork()\n",
    "\n",
    "        self.actor_optimizer = Adam(learning_rate=lr_actor)\n",
    "        self.critic_optimizer = Adam(learning_rate=lr_critic)\n",
    "\n",
    "    def get_action(self, state): # it will take state generated by reset function, pass it to the actor network \n",
    "        #and returns an action Get action probabilities from the actor network\n",
    "        \n",
    "        action_probs = self.actor(np.array([state]))\n",
    "        action = np.random.choice(self.n_actions, p=action_probs.numpy()[0])\n",
    "        return action, action_probs\n",
    "\n",
    "    def train(self, states, actions, discounted_rewards): \n",
    "        '''\n",
    "          It will take state generated by the reset function, action predicted by the actor network and its cummulative reward\n",
    "          \n",
    "         it should take state and action taken by actor and use them to calculate max cummulative reward and use temporal\n",
    "         difference to update the weights of actor and its own network\n",
    "        \n",
    "        it is for training of both the neural networks and does not return any value as such\n",
    "        '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6fdfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CarEnv:\n",
    "    def __init__(self, target_position=10.0, obstacle_positions=[4.0, 6.0]):\n",
    "        self.target_position = target_position\n",
    "        self.obstacle_positions = obstacle_positions\n",
    "        self.state_dim = 2  # Position, velocity\n",
    "        self.action_dim = 1  # Continuous action for acceleration\n",
    "        self.current_position = 0.0\n",
    "        self.current_velocity = 0.0\n",
    "        self.max_velocity = 2.0\n",
    "        self.min_velocity = -2.0\n",
    "\n",
    "    def reset(self): # initializes or resets the environment and creates a state\n",
    "        '''\n",
    "        1. get the world map and spawn vehicle at random point assigned by the environment\n",
    "        2. choose another random or selected point as target value\n",
    "        3. initialize camera sensor and attach it to the vehicle\n",
    "        4. initialize lidar and attach it to the vehicle\n",
    "        5. initialize IMU and attach it to the vehicle\n",
    "        6. initialize GNSS and attach it to the vehicle\n",
    "        7. see if lane invasion and traffic light sensors needs to be incorporated or not\n",
    "        return state\n",
    "        '''\n",
    "        return np.array([self.current_position, self.current_velocity])\n",
    "\n",
    "    def step(self, action): # this function is called after the actor network's action prediction\n",
    "        \n",
    "    ''' takes action predicted by the actor network\n",
    "    \n",
    "        This function in a reinforcement learning environment simulates one timestep of interaction between the \n",
    "        agent and the environment. It takes an action as input, updates the environment based on that action, \n",
    "        calculates the reward for the action, and provides the new state and reward to the agent.'''\n",
    "        '''\n",
    "        1. The actor has taken an action and based on that action a new state has been created which is generalized here and sent\n",
    "        as a new_state variable\n",
    "        the action taken by the actor will now creates a new state\n",
    "        \n",
    "        2. The reward function will be developed here to reward or penalize the model based on the action and new state\n",
    "        \n",
    "        3. also it will be checked if the model is done or not, this will be used to terminate model if collision occurs\n",
    "       \n",
    "        return new_state, reward, done, {}\n",
    "        \n",
    "        Question is how we will be sending multiple actions like acceleration and steering as a single action ?\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DRL Actor-Critic agent\n",
    "class DRLActorCritic:\n",
    "    # ... (previously defined code for DRLActorCritic)\n",
    "\n",
    "# Define the environment\n",
    "env = CarEnv()\n",
    "\n",
    "# Initialize the DRL Actor-Critic agent\n",
    "state_dim = 2  # will depend on images+sensors dataset\n",
    "n_actions = 1  # will depend upon how we want the action\n",
    "agent = DRLActorCritic(state_dim, n_actions)\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 1000\n",
    "\n",
    "# Episodic training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()  # Reset the environment to start a new episode\n",
    "    done = False  # Initialize the episode termination flag\n",
    "    total_reward = 0  # Initialize the total reward for this episode\n",
    "\n",
    "    while not done:\n",
    "        # Choose an action using the current policy (actor network)\n",
    "        action, action_probs = agent.get_action(state)\n",
    "\n",
    "        # Take the chosen action and observe the next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Train the agent using the observed transition (s, a, r, s')\n",
    "        agent.train(np.array([state]), np.array([action]), np.array([reward]))\n",
    "\n",
    "        # Update the current state for the next iteration\n",
    "        state = next_state\n",
    "\n",
    "        # Accumulate the reward for this episode\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Close the environment (if needed)\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b08836",
   "metadata": {},
   "source": [
    "In this episodic training loop:\n",
    "\n",
    "We reset the environment at the beginning of each episode using env.reset().<br>\n",
    "Within each episode, we iterate until the episode termination condition (done) is met.<br>\n",
    "For each timestep within an episode:<br>\n",
    "a. We choose an action using the current policy (agent.get_action(state)).<br>\n",
    "b. We take the chosen action and observe the next state and reward (env.step(action)).<br>\n",
    "c. We train the agent using the observed transition (agent.train(...)).<br>\n",
    "d. We update the current state for the next iteration.<br>\n",
    "e. We accumulate the reward for this episode.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a73af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b77772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7723082c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
